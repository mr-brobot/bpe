{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xef\\xbc\\xb5\\xef\\xbd\\x8e\\xef\\xbd\\x89\\xef'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.reedbeta.com/blog/programmers-intro-to-unicode/\n",
    "text = \"Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\"\n",
    "\n",
    "text.encode(\"utf-8\")[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\n",
      "length: 533\n",
      "---\n",
      "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 101, 32, 118, 101, 114, 121, 32, 110, 97, 109, 101, 32, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 101, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 101, 32, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 101, 32, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 101, 32, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 101, 32, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 101, 32, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 101, 32, 99, 97, 110, 32, 98, 101, 32, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 101, 32, 85, 110, 105, 99, 111, 100, 101, 32, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 101, 32, 109, 111, 114, 101, 32, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 101, 32, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 101, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 101, 32, 119, 104, 111, 108, 101, 32, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
      "length: 616\n"
     ]
    }
   ],
   "source": [
    "tokens = list(map(int, text.encode(\"utf-8\")))\n",
    "\n",
    "print(\"---\")\n",
    "print(text)\n",
    "print(f\"length: {len(text)}\")\n",
    "print(\"---\")\n",
    "print(tokens)\n",
    "print(f\"length: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should we identify byte pairs? Or grapheme cluster pairs?\n",
    "\n",
    "BPE implies byte-level tokenization, so that is what we will implement here.\n",
    "\n",
    "However, grapheme cluster pair encoding (GCPE) might be more meaningful since it operates at the \"character\" level, i.e., the \"perceptual atom\" of text.\n",
    "\n",
    "It might be the case that, as BPE begins to iteratively \"mint tokens\", new tokens begin to align with grapheme clusters.\n",
    "\n",
    "Either way, Unicode normalization is important to ensure that the binary representation of text is in a consistent form before tokenization. This is beneficial for BPE, but significantly more important for grapheme cluster tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Collection\n",
    "from collections import Counter\n",
    "\n",
    "def most_common_pair(tokens: Collection[int]) -> Tuple[int, int]:\n",
    "    pairs = zip(tokens[:-1], tokens[1:])\n",
    "    counter = Counter(pairs)\n",
    "    return counter.most_common(1)[0][0]\n",
    "\n",
    "assert most_common_pair([2, 3, 2, 3, 4]) == (2, 3)\n",
    "assert most_common_pair([1, 2, 1, 2, 3, 4, 3]) == (1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('e', ' ')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcp = most_common_pair(tokens)\n",
    "(chr(mcp[0]), chr(mcp[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_pair(tokens: Collection[int], pair: Tuple[int, int], replacement: int) -> Collection[int]:\n",
    "    result = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if i + 1 < len(tokens) and (tokens[i], tokens[i + 1]) == pair:\n",
    "            result.append(replacement)\n",
    "            i += 2\n",
    "        else:\n",
    "            result.append(tokens[i])\n",
    "            i += 1\n",
    "    return result\n",
    "\n",
    "assert replace_pair([2, 3, 2, 3, 4], (2, 3), 5) == [5, 5, 4]\n",
    "assert replace_pair([1, 2, 1, 1, 3, 1, 2], (1, 2), 4) == [4, 1, 1, 3, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though we are replacing instances of tokens with \"minted\" tokens, it is important that we retain the original set of tokens. We are replacing instances, not modifying the vocabulary.\n",
    "\n",
    "Replacing pairs should not restrict the use of the pair members in the future. For example, if BPE eliminate all usages of a specific character, e.g., \"t\",  we would not be able to use \"t\" in a novel way in the future if we lose the vocab entry for \"t\" in the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of building a \"forest\" of tokens and merging towards larger trees is reminiscent of Huffman coding. Huffman coding aims to turn a \"forest\" into a single tree. BPE aims to produce a forest with a target number of nodes across all trees.\n",
    "\n",
    "Huffman coding also operates on individual elements, while BPE operates on sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "from typing import Collection\n",
    "\n",
    "def utf8_encode(text: str) -> Collection[int]:\n",
    "    normalized = unicodedata.normalize(\"NFD\", text)\n",
    "    return list(map(int, normalized.encode(\"utf-8\")))\n",
    "\n",
    "def utf8_decode(tokens: Collection[int]) -> str:\n",
    "    encoded = bytes(tokens)\n",
    "    return encoded.decode(\"utf-8\")\n",
    "\n",
    "assert utf8_decode(utf8_encode(\"hello\")) == \"hello\"\n",
    "assert utf8_decode(utf8_encode(text)) == text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: 616\n",
      "min: 32\n",
      "max: 240\n",
      "distinct: 73\n"
     ]
    }
   ],
   "source": [
    "tokens = utf8_encode(text)\n",
    "print(f\"length: {len(tokens)}\")\n",
    "print(f\"min: {min(tokens)}\")\n",
    "print(f\"max: {max(tokens)}\")\n",
    "print(f\"distinct: {len(set(tokens))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Collection\n",
    "\n",
    "Vocab = Collection[int]\n",
    "Merges = Collection[Tuple[int, int]]\n",
    "\n",
    "def bpe_train(text: str, vocab_size: int) -> Tuple[Vocab, Merges]:\n",
    "    \"\"\"\n",
    "    output:\n",
    "        - vocab: array of base tokens, where index is token id\n",
    "        - merges: array of merged token pairs, where len(vocab) + index is token id\n",
    "    \"\"\"\n",
    "    tokens = utf8_encode(text)\n",
    "\n",
    "    # dict-encoded vocab\n",
    "    vocab = sorted(set(tokens))\n",
    "\n",
    "    # encode tokens with vocab\n",
    "    vocab_index = {token: i for i, token in enumerate(vocab)}\n",
    "    tokens = [vocab_index[token] for token in tokens]\n",
    "\n",
    "    num_merges = vocab_size - len(vocab)\n",
    "    merges = []\n",
    "    i = len(vocab)\n",
    "    while len(merges) < num_merges:\n",
    "        pair = most_common_pair(tokens)\n",
    "        merges.append(pair)\n",
    "\n",
    "        tokens = replace_pair(tokens, pair, i)\n",
    "        i += 1\n",
    "\n",
    "    return vocab, merges\n",
    "\n",
    "def bpe_encode(text: str, vocab: Vocab, merges: Merges) -> Collection[int]:\n",
    "    tokens = utf8_encode(text)\n",
    "\n",
    "    # encode tokens with vocab\n",
    "    vocab_index = {token: i for i, token in enumerate(vocab)}\n",
    "    tokens = [vocab_index[token] for token in tokens]\n",
    "\n",
    "    for i, merge in enumerate(merges):\n",
    "        pairs = set(zip(tokens[:-1], tokens[1:]))\n",
    "        if merge in pairs:\n",
    "            tokens = replace_pair(tokens, merge, len(vocab) + i)\n",
    "    return tokens\n",
    "\n",
    "def bpe_decode(tokens: Collection[int], vocab: Vocab, merges: Merges) -> str:\n",
    "    # each merge can only depend on vocab or previous merges\n",
    "    vm_index = [[t] for t in vocab]\n",
    "    for pair in merges:\n",
    "        result = vm_index[pair[0]] + vm_index[pair[1]]\n",
    "        vm_index.append(result)\n",
    "\n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        result.extend(vm_index[token])\n",
    "    return utf8_decode(result)\n",
    "\n",
    "vocab, merges = bpe_train(text, 256)\n",
    "encoded = bpe_encode(text, vocab, merges)\n",
    "assert bpe_decode(encoded, vocab, merges) == text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 73\n",
      "merges size: 183\n",
      "raw length: 616\n",
      "encoded length: 197\n",
      "compression: 3.1269035532994924\n"
     ]
    }
   ],
   "source": [
    "print(f\"vocab size: {len(vocab)}\")\n",
    "print(f\"merges size: {len(merges)}\")\n",
    "print(f\"raw length: {len(utf8_encode(text))}\")\n",
    "print(f\"encoded length: {len(encoded)}\")\n",
    "print(f\"compression: {len(tokens) / len(encoded)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "1. implement bpe in neuralink compression challenge notebook\n",
    "2. implement optional rules parameter for bpe training (review gpt3/4 rules)\n",
    "3. implement minbpe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
